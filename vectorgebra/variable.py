from .infinity import *

"""

    Below 2 functions and the constant are redefined, because of
    the import chain problems.

"""

log2E = 1.4426950408889392

def log2(x: Union[int, float, Decimal, Infinity, Undefined], resolution: int = 15):
    """
        Computes the base-2 logarithm of the given number.

        Args:
            x: The number.
            resolution (int, optional): The resolution for the approximation. Defaults to 15.

        Returns:
            float: The base-2 logarithm of the given number.

        Raises:
            ArgTypeError: If 'x' is not a numerical value.
            RangeError: If 'x' is less than or equal to 0, or if 'resolution' is not a positive integer.
    """
    if not isinstance(x, Union[int, float, Decimal, Infinity, Undefined]):
        raise ArgTypeError("Must be a numerical value.")
    if x <= 0:
        raise RangeError()
    if resolution < 1:
        raise RangeError()
    # finally...
    count = 0
    factor = 1
    if x < 1:
        factor = -1
        x = 1 / x

    while x > 2:
        x /= 2
        count += 1

    # x can be a decimal
    for i in range(1, resolution + 1):
        x *= x
        if x >= 2:
            count += 1 / (2**i)
            x /= 2

    return factor * count

def ln(x: Union[int, float, Decimal, Infinity, Undefined], resolution: int = 15):
    """
        Computes the base-e logarithm of the given number.

        Args:
            x: The number.
            resolution (int, optional): The resolution for the approximation. Defaults to 15.

        Returns:
            float: The base-e logarithm of the given number.

        Raises:
            ArgTypeError: If 'x' is not a numerical value.
            RangeError: If 'x' is less than or equal to 0, or if 'resolution' is not a positive integer.
        """
    return log2(x, resolution) / log2E

class BinaryNode:

    def __init__(self, val, var):
        self.data = val  # CONSTANT
        self.variable = var
        self.left = None
        self.right = None
        self.parent = None

    def alone(self):
        return (self.left is None or not self.left.flag) and (self.right is None or not self.right.flag)


class Variable:

    """
        A basic numerical variable, that forms a computational graph as
        an extra.

    """

    def __init__(self, val):
        self.value = val
        self.forward = None
        self.backward = None
        self.operation = None  # Operation to apply to self.backward
        self.id = id(self)  # To prevent repetitive function calls during backpropagation

    def __str__(self):
        return self.value.__str__()

    def __repr__(self):
        return self.value.__repr__()

    def __getitem__(self, item):
        return self.value.__getitem__(item)

    def __setitem__(self, key, value):
        return self.value.__setitem__(key, value)

    def search(self, ID):
        """
            Find a specific node on the computational graph. Finds
            the node by its native object ID. Assumes the node is
            unique on the graph. Returns the latest occurrence if
            the node appears multiple times on the graph.

            Args:
                ID (int): Object ID generated by CPython to look for
                    in the graph.

            Returns:
                The Variable object of the found node if it is found.
                Otherwise returns None. Check for "None" case, for
                error handling if needed.

        """
        if self.id == ID:
            if self.backward is not None:
                result1 = self.backward[0].search(ID)
                result2 = self.backward[1].search(ID)
                if result1 is not None:
                    return result1
                if result2 is not None:
                    return result2
            return self
        else:
            if self.backward is not None:
                result1 = self.backward[0].search(ID)
                result2 = self.backward[1].search(ID)
                if result1 is not None:
                    return result1
                if result2 is not None:
                    return result2

    def propagate(self, ID):
        """
            Forward propagate the operations to calculate
            the gradient. If the variable to take the derivative
            respect to appears only once on the computational
            graph, this function is faster for calculating the
            derivative than the backpropagation.

            Args:
                ID (int): Object ID generated by CPython of "self".
                    If not "self", may cause errors.

            Returns:
                The derivative of the computation respect to object
                with the ID.

            Notes:
                This method needs to be called on the node that hosts
                the variable to take the derivative respect to. This
                method is designed more primitively, compared to other
                methods and functions on this class and its helpers.

        """
        if self.forward is None:
            if self.operation == "ADD":
                return 1

            if self.operation == "MUL":
                if self.backward[0].id == ID:
                    return self.backward[1].value
                else:
                    return self.backward[0].value

            if self.operation == "POW":
                if self.backward[0].id == ID:
                    return self.backward[1].value * (
                                self.backward[0].value ** (self.backward[1].value - 1))
                else:
                    return self.value * ln(self.backward[0].value) * self.propagate(self.id)

            if self.operation == "DIV":
                if self.backward[0].id == ID:
                    return 1 / self.backward[1].value
                else:
                    return -self.backward[0].value / (self.backward[1].value ** 2)

        if self.operation == "ADD":
            return self.forward.propagate(self.id)

        if self.operation == "MUL":
            if self.backward[0].id == ID:
                return self.backward[1].value * self.forward.propagate(self.id)
            else:
                return self.backward[0].value * self.forward.propagate(self.id)

        if self.operation == "POW":
            if self.backward[0].id == ID:
                return self.backward[1].value * (self.backward[0].value ** (self.backward[1].value - 1)) * self.forward.propagate(self.id)
            else:
                return self.value * ln(self.backward[0].value) * self.propagate(self.id)

        if self.operation == "DIV":
            if self.backward[0].id == ID:
                return self.forward.propagate(self.id) / self.backward[1].value
            else:
                return -self.backward[0].value * self.forward.propagate(self.id) / (self.backward[1].value ** 2)

        return self.forward.propagate(self.id)

    def backpropagate(self, ID):
        """
            Backpropagate the computational graph to compute the
            gradient.

            Args:
                 ID (int): The Object ID generated by CPython of
                    the node to take the gradient respect to.

            Returns:
                The gradient as a value. If you modify Variable
                objects manually, there is a chance that this
                method returns None. Recursive calls of this
                method on the graph may raise exceptions because
                of that.
        """

        if self.id == ID:
            return 1

        if self.backward is None:
            return 0

        if self.operation == "ADD":
            return self.backward[0].backpropagate(ID) + self.backward[1].backpropagate(ID)

        if self.operation == "MUL":
            return self.backward[0].backpropagate(ID) * self.backward[1].value + \
                self.backward[0].value * self.backward[1].backpropagate(ID)

        if self.operation == "POW":
            return self.backward[0].backpropagate(ID) * self.backward[1].value * (self.backward[0].value ** (self.backward[1].value - 1)) + \
                ln(self.backward[0].value) * (self.backward[0].value ** self.backward[1].value) * self.backward[1].backpropagate(ID)

        if self.operation == "DIV":
            return self.backward[0].backpropagate(ID) / self.backward[1].value - self.backward[1].backpropagate(ID) * self.backward[0].value / (self.backward[1].value ** 2)

    def derive(self):

        if self.operation == "ADD":
            return (1, 1), self.backward
        if self.operation == "MUL":
            return (self.backward[1].value, self.backward[0].value), self.backward
        if self.operation == "POW":
            return ((self.backward[1].value * (self.backward[0].value ** (self.backward[1].value - 1)),
                    ln(self.backward[0].value) * (self.backward[0].value ** self.backward[1].value)),
                    self.backward)
        if self.operation == "DIV":
            return (1/self.backward[1].value, -self.backward[0].value / (self.backward[1].value ** 2)), self.backward

        return (1, 1), (0, 0)

    def set(self, val):
        """
            Change, reset the value that is wrapped by Variable.
            This method is not used for changing inner values held
            by the self.value itself, for example when wrapping a
            Vector.

            Args:
                val: New value to reassign to the Variable node, self.

        """
        if self.backward is None:
            self.value = val
        else:
            raise ArithmeticError()

    def calculate(self):
        """
            (Re)calculate the computational graph, backpropagating
            from the self node. This method potentially changes the
            value stored in self.value, numerically.

            Returns:
                The value calculated to be stored in node, self.
                This method may potentially raise an ArithmeticError
                if Variable objects are manipulated manually.

        """
        if self.backward is None:
            val = self.value
        elif self.operation == "ADD":
            val = self.backward[0].calculate() + self.backward[1].calculate()
        elif self.operation == "MUL":
            val = self.backward[0].calculate() * self.backward[1].calculate()
        elif self.operation == "POW":
            val = self.backward[0].calculate() ** self.backward[1].calculate()
        elif self.operation == "DIV":
            val = self.backward[0].calculate() / self.backward[1].calculate()
        else:
            raise ArithmeticError()
        self.value = val
        return val

    def __add__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(self.value + arg.value)
        self.forward = result
        arg.forward = result

        result.backward = (self, arg)
        result.operation = "ADD"
        return result

    def __radd__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(self.value + arg.value)
        self.forward = result
        arg.forward = result

        result.backward = (self, arg)
        result.operation = "ADD"
        return result

    def __sub__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(self.value - arg.value)
        self.forward = result
        arg.forward = result

        result.backward = (self, -arg)
        result.operation = "ADD"
        return result

    def __rsub__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(arg.value - self.value)
        self.forward = result
        arg.forward = result

        result.backward = (arg, -self)
        result.operation = "ADD"
        return result

    def __mul__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(self.value * arg.value)
        self.forward = result
        arg.forward = result

        result.backward = (self, arg)
        result.operation = "MUL"
        return result

    def __rmul__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(self.value * arg.value)
        self.forward = result
        arg.forward = result

        result.backward = (arg, self)
        result.operation = "MUL"
        return result

    def __truediv__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(self.value / arg.value)
        self.forward = result
        arg.forward = result

        result.backward = (self, arg)
        result.operation = "DIV"
        return result

    def __rtruediv__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(arg.value / self.value)
        self.forward = result
        arg.forward = result

        result.backward = (arg, self)
        result.operation = "DIV"
        return result

    def __neg__(self):
        return -1 * self

    def __pos__(self):
        return self

    def __pow__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(self.value ** arg.value)
        self.forward = result
        arg.forward = result

        result.backward = (self, arg)
        result.operation = "POW"
        return result

    def __rpow__(self, arg):
        if not isinstance(arg, Variable):
            arg = Variable(arg)

        result = Variable(self.value ** arg.value)
        self.forward = result
        arg.forward = result

        result.backward = (arg, self)
        result.operation = "POW"
        return result

    def __ge__(self, arg):
        if isinstance(arg, Variable):
            return self.value >= arg.value
        return self.value >= arg

    def __gt__(self, arg):
        if isinstance(arg, Variable):
            return self.value > arg.value
        return self.value > arg

    def __le__(self, arg):
        if isinstance(arg, Variable):
            return self.value <= arg.value
        return self.value <= arg

    def __lt__(self, arg):
        if isinstance(arg, Variable):
            return self.value <= arg.value
        return self.value <= arg

    def __eq__(self, arg):
        if isinstance(arg, Variable):
            return self.value == arg.value
        return self.value == arg

    def __ne__(self, arg):
        if isinstance(arg, Variable):
            return self.value != arg.value
        return self.value != arg

    def __or__(self, arg):
        if isinstance(arg, Variable):
            return self.value or arg.value
        return self.value or arg

    def __ror__(self, arg):
        if isinstance(arg, Variable):
            return self.value or arg.value
        return self.value or arg

    def __and__(self, arg):
        if isinstance(arg, Variable):
            return self.value and arg.value
        return self.value and arg

    def __rand__(self, arg):
        if isinstance(arg, Variable):
            return self.value and arg.value
        return self.value and arg

    def __xor__(self, arg):
        if isinstance(arg, Variable):
            return self.value ^ arg.value
        return self.value ^ arg

    def __rxor__(self, arg):
        if isinstance(arg, Variable):
            return self.value ^ arg.value
        return self.value ^ arg


def grad(node: Variable, args: Union[List[Variable], None] = None):
    """
        Takes the gradient of the given computational graph,
        respect to given Variable list.

        Args:
            node (Variable): The last Variable that results
                from the computation, that the gradient will
                be calculated on.

            args (List[Variable]): A list of Variable's that
                the gradient will be calculated respect to.

        Returns:
            A list of values representing the gradients. The
            order of the values correspond to the "args".
            Therefore this list is also a gradient vector/list
            when given multiple arguments.

        Raises:
            ArgTypeError: When args argument is left blank.
    """
    if args is None:
        raise ArgTypeError("Cannot take derivative respect to None.")

    tree_traverser: BinaryNode
    root = BinaryNode(1, node)
    node_frontier = [root]
    replace_node_frontier: list

    id_node_dict = {arg.id: 0 for arg in args}
    ids = id_node_dict.keys()

    # Build the pseudo-computational graph for the values of the derivatives
    while node_frontier:
        replace_node_frontier = []

        for temp_node in node_frontier:

            if temp_node.variable.id in ids:
                id_node_dict[temp_node.variable.id] += temp_node.data

            values, children = temp_node.variable.derive()

            if not isinstance(children[0], int):
                temp_node.left = BinaryNode(values[0] * temp_node.data, children[0])
                temp_node.left.parent = temp_node
                temp_node.right = BinaryNode(values[1] * temp_node.data, children[1])
                temp_node.right.parent = temp_node
                replace_node_frontier.append(temp_node.left)
                replace_node_frontier.append(temp_node.right)

        node_frontier = replace_node_frontier

    return list(id_node_dict.values())


